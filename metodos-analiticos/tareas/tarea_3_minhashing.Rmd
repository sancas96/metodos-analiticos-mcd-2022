---
title: 'Tarea 2: minhashing y LSH para textos'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Encontrando duplicados exactos

Si queremos deduplicar una colección datos, podemos hacer algo simple:
formamos una "llave" con todo el documento, y con esa llave creamos cubetas. Por ejemplo

```{r}
library(tidyverse)
textos <- tibble(doc_id = 1:5, 
                 doc = c("una manzana es una fruta", 
                         "una pera", "una manzana es una fruta", 
                         "una uva", "una pera..."))
textos
```

Usamos como llave al documento y agrupamos:

```{r}
textos |> group_by(doc) |> 
  summarise(docs = list(doc_id)) |> 
  mutate(docs = map_chr(docs, ~ str_flatten(.x, collapse = ",")))
```
Y como vemos, hemos encontrado que el documento 1 y el 3 son idénticos, sin tener
que comparar todos los pares de documentos.

### Encontrando duplicados exactos: codificando con un hash

Como los documentos pueden ser grandes y ocupar mucha memoria, podemos construir
una función hash, que cumpla:

- La función toma un documento y produce un entero grande pero de tamaño fijo.
- Si dos documentos son diferentes, entonces es muy poco probable que tengan el
mismo hash  (la probabilidad de colisión es baja).
- La función es determinística, es decir, siempre produce el mismo hash con la misma entrada
(como cualquier función usual).

Podemos usar la función *digest2int* del paquete digest 
(si usas python puedes usar un hash murmur32 o algón otro que sea rápido, no criptográfico):

```{r}
library(digest)
# producir valor hash de la cadena "hola"
digest2int("hola")
```

Nota: quizá conoces otros hashes como md5 o sha-256, etc. Este tipo de hashes está
más orientado a la criptografía, y por esa aplicación, puede ser comparativamente
lento usar estos hashes vs otros que no son criptográficos.

**Pregunta 1**: Prueba poniendo distintas cadenas en esta función. ¿Puedes ver algún
patrón que ligue la entrada con valor hash producido? ¿Es cierto que si cambias poco
la cadena de entrada la salida cambia poco?


Aplicando este hash a nuestros documentos completos:

```{r}
hash_textos <- textos |> 
  mutate(hash = digest2int(doc)) |> 
  select(doc_id, hash)
hash_textos
```

Y ahora podemos agrupar hashes iguales para enocontrar los documentos idénticos:

```{r}
hash_textos |> 
  group_by(hash) |> 
  summarise(docs = list(doc_id)) |> 
  mutate(docs = map_chr(docs, ~ str_flatten(.x, collapse = ",")))
```
Y resolvimos el mismo problema, pero sin tener que usar una llave (el texto) que puede ocupar mucha memoria. Siempre y cuando la función hash que usemos tenga pocas colisiones, este método funciona.

Nótese que aún cuando haya algunas colisiones, podemos checar los pares que resultaron,
y verificar que si en efecto son el mismo documento caracter por caracter. Con este método,
el número de estos chequeos es muy pequeño si la fracción de duplicados no es muy grande, y
este proceso resulta más eficiente.


**Ahora supongamos que queremos encontrar documentos que son muy similares, aunque no
exactamente iguales necesariamente.**

**Pregunta 2**: Explica por qué el enfoque de arriba no funciona para el problema
de documentos muy similares. ¿Qué pasa con hashes de documentos que son muy similares
pero no exactamente iguales? 


### Similitud de jaccard y minhash

Para resolver este problema, buscamos entonces construir una función tal
que cuando dos documentos son muy similares (similitud de Jaccard de tejas),
entonces es altamente probable que la función mapee los dos documentos al mismo hash.

Una idea simple es considerar los hashes de las tejas de cada documento, y
tomar el mínimo (aunque podríamos también tomar el máximo por ejemplo) de esos valores.

- Si dos documentos son muy similares, entonces es probable que el mínimo hash de sus
tejas ocurra en una teja que tienen ambos documentos, y así, tendrán el mismo hash mínimo o
minhash

```{r}
calcular_tejas <- function(x, k){
  tokenizers::tokenize_character_shingles(x, n = k, lowercase = FALSE,
    simplify = TRUE, strip_non_alpha = FALSE)
}
textos <- tibble(doc_id = 1:5, 
                 doc = c("una manzana es una fruta", 
                         "una pera", "una mnzana es una fruta", 
                         "algunas uvas verdes", "una pera..."))
textos
```
```{r}
tejas_tbl <- textos |> mutate(tejas = map(doc, ~ calcular_tejas(.x, k = 3)))
tejas_tbl
```

Las tejas son:

```{r}
tejas_tbl$tejas
```


Aplicamos el hash a cada teja:

```{r}
hashes <- map(tejas_tbl$tejas, ~ digest2int(.x))
hashes
```

Y sacamos el mínimo en cada documento:

```{r}
map(hashes, ~ min(.x))
```

Nótese ahora que aún cuando el documento 1 y 3 no son idénticos, su minhash es igual.

**Pregunta 3**: ¿Por qué el minhash de 1 y 3 son iguales? ¿Exactamente en qué teja ocurre el mínimo para cada uno de estos documentos? Explica por qué si los dos documentos son muy diferentes, entonces es poco probable el mínimo ocurra en una teja común.

### Agrupando con minhashes

Ahora repetimos los cálculos con la tabla y agrupamos:

```{r}
minhashes_tbl <- tejas_tbl |> 
  mutate(hashes = map(tejas, ~ digest2int(.x))) |> 
  mutate(minhash = map_int(hashes, ~ min(.x))) |> 
  select(doc_id, minhash)
minhashes_tbl
```

```{r}
minhash_cubetas_tbl <- minhashes_tbl |> 
  group_by(minhash) |> 
  summarise(docs_lista = list(doc_id)) |> 
  mutate(docs = map_chr(docs_lista, ~ str_flatten(.x, collapse = ",")))
minhash_cubetas_tbl
```

Y así pudimos agrupar documentos muy similares, sin tener que hacer todas las comparaciones
de los pares posibles.

**Pregunta 4**: esta última tabla se llama la tabla de **candidatos de similitud alta**. Explica por qué debemos considerarlos como candidatos. ¿Cuándo puede ser que dos documentos que son poco similares tengan el mismo minhash? Es decir, cómo puede pasar que tengamos un
*falso positivo* con este proceso?

**Pregunta 5**: Con este método también podemos obtener *falsos negativos*, es decir, no capturar pares muy similares como candidatos en nuestra tabla. ¿Cómo puede pasar esto
en término de tejas y hashes?

Estos dos problemas podemos resolverlos extendiendo este mismo método, como veremos 
después.


### Búsqueda rápida de vecinos cercanos.

Supongamos ahora que tenemos una cadena nueva, y queremos ver si es un duplicado
cercano de algún elemento en nuestra colección de textos.

**Pregunta 6**: Explicacómo convirtiendo a tejas y calculando el minhash del documento
nuevo puedes encontrar rápídamente si el nuevo documento tiene un documento **muy** similar
en la colección de textos anterior. NOTA: este proceso tiene falsos positivos y falsos negativos, así que prueba con distintos textos nuevos muy similares a los anteriores.


### Reduciendo falsos positivos

Supongamos que queremos capturar documentos con similitud mayor a 0.7. Entonces
podemos **eliminar** falsos positivos calculando la similitud exacta para cada par candidato.

En nuestro ejemplo, calculamos la similitud entre 1 - 3 y 2 - 5

```{r}
sim_jaccard <- \(a, b)  length(intersect(a, b)) / length(union(a, b))
sim_jaccard(tejas_tbl$tejas[[1]], tejas_tbl$tejas[[3]])
sim_jaccard(tejas_tbl$tejas[[2]], tejas_tbl$tejas[[5]])
```
Y verificamos que el par 1-3 es efectivamente un positivo, pero el par 3-5 queda
un poco corte del punto de corte de similitud que establecimos. En este caso,
eliminaríamos este par de los candidatos.

**Pregunta 7**: considera que tienes una colección típica de texto. Si hacemos este
proceso, encontraremos varios pares candidatos. Para identificar los verdaderos
positivos, tenemos que calcular la similitud exacta de los candidatos. Expica por qué
típicamente esto es mucho menos trabajo que calcular la similitud de todos los 
pares posibles.


### Reduciendo falsos negativos

Algunas veces, queremos encontrar pares de similitud un poco más baja, y el proceso de arriba
puede fallar en identificar esos pares.

Por ejemplo, considera los siguientes textos:

```{r}
textos <- tibble(doc_id = 1:5, 
                 doc = c("un plátano no es una pera", 
                         "frutas", 
                         "dijeron que: el plátano no es pera", 
                         "algunas uvas verdes", "unas frutas."))
textos
# Calcular tejas
tejas_tbl <- textos |> mutate(tejas = map(doc, ~ calcular_tejas(.x, k = 3)))
# Calcular minhashes
minhashes_tbl <- tejas_tbl |> 
  mutate(hashes = map(tejas, ~ digest2int(.x))) |> 
  mutate(minhash = map_int(hashes, ~ min(.x))) |> 
  select(doc_id, minhash)
minhashes_tbl
```
En este caso, los documentos 1 y 3 no serán identificados como candidatos porque
su minhash es diferente. Para capturar pares de similitud menor, podemos usar
otros minhashes diferentes generados al azar.

La idea es agrupar un par cuando **cualquiera de sus minhashes coinciden**:

```{r}
set.seed(199)
generar_minhash <- function(){
  r <- as.integer(stats::runif(1, 1, 2147483647))
  funcion_minhash <- function(tejas){
        min(digest::digest2int(tejas, seed = r)) 
  }
  funcion_minhash
}
# esta lista contiene tres funciones minhash diferentes generadas al azar
minhashes <- map(1:3, ~ generar_minhash())
```

En este caso usamos tres minhashes

```{r}
minhashes_tbl <- tejas_tbl |> 
  mutate(minhashes = map(tejas, ~ map_int(minhashes, \(f_mhash) f_mhash(.x)))) |> 
  select(doc_id, minhashes) |> 
  unnest(minhashes) |> 
  group_by(doc_id) |> 
  mutate(n_hash = 1:3) 
minhashes_tbl
```

Nótese ahora que uno de los tres mihashes para el documento 1 y 3 coinciden,
de forma que los agruparemos como candidatos:

```{r}
minhash_cubetas_tbl <- 
  minhashes_tbl |> 
  mutate(cubeta_minhashes = paste0(n_hash, minhashes)) |> 
  select(cubeta_minhashes, doc_id) |> 
  group_by(cubeta_minhashes) |> 
  summarise(docs_lista = list(doc_id)) |> 
  mutate(docs = map_chr(docs_lista, ~ str_flatten(.x, collapse = ",")))
minhash_cubetas_tbl
```

Nuestros candidatos finales son:

```{r}
minhash_cubetas_tbl |> 
  mutate(n_docs = map_int(docs_lista, length)) |> 
  filter(n_docs > 1)
```

Y ahora capturamos el par 1-3 que en nuestro análisis anterior no habíamos capturado

**Pregunta 8**: ¿Qué método tiende a producir más pares, el que usa tres minhashes o el
que usa un solo minhash? Explica por qué esto tiende a reducir la tasa de falsos negativos
(pares que nos capturamos como candidatos), pero también potencialmente aumentar la tasas
de falsos positivos.

**Pregunta 10**: ¿Qué pasa si en lugar de pedir que alguno de los tres minhashes
coincida, pedimos que los tres minhashes coincidan? Tenderías a obtener menos pares
o más pares candidatos?

**Pregunta 11** (más difícil, opcional) Supón que la similitud de Jaccard de dos documentos
es $s$. ¿Cuál es la probabilidad de capturarlo como par candidatos con un solo minhash?
Suponiendo que los tres minhashes se escogen independientemente, ¿Cuál es la probabilidad
de capturar un par de similitud $s$ si pedimos que al menos uno de los tres minhashes coincida? ¿Cuál es la probabilidad si pedimos que los tres minhashes coincidan?






