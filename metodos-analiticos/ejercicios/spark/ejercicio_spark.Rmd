---
title: "Introducción a sparklyr"
output: html_notebook
---

En esta parte veremos algunos aspectos básicos de Spark, que nos servirá
para escalar análisis (tanto a máquinas con muchos cores y memoria como
a clusters para los problemas más grande). En primer lugar, notamos que
cuando es posible, es más simple y rápido usar máquinas más grandes y
trabajar con las herramientas más flexibles que conmunmente utilizamos
(pandas y dplyr), ver por ejemplo
[aquí](https://h2oai.github.io/db-benchmark/), o utilizar bases de datos
tradicionales, ver por ejemplo [esta
plática](https://databricks.com/session/not-your-fathers-database-how-to-use-apache-spark-properly-in-your-big-data-architecture).

## Conectarse a un cluster

Nos conectamos a una instancia local de Spark:

```{r, message = FALSE}
library(tidyverse)
library(sparklyr)
config <- spark_config()
# configuración para modo local, ajustar la memoria del driver solamente
config$`sparklyr.shell.driver-memory` <- "2G"
# esta línea es necesaria para que la ui funcione en el contenedor:
#config$`spark.env.SPARK_LOCAL_IP.local` <- "0.0.0.0"
config$`sparklyr.connect.cores.local` <- 8
#config$spark.sql.shuffle.partitions.local <- 100
sc <- spark_connect(master = "local")
sc$conf
```

## Cargar datos a Spark

Normalmente los datos están en algún lugar, quizá en un sistema
distribuido de archivos como HDFS. En nuestro caso están en nuestro
sistema de archivos local:

```{bash}
wc -l ../../datos/profiles.csv
head -2 ../../datos/profiles.csv
```

Podemos cargar a Spark (sin pasar por R) como sigue:

```{r}
perfiles_tbl <- spark_read_csv(sc, 
  path = "../../datos/profiles.csv",
  name = "perfiles",
  escape = "\"", 
  options = list(multiline = TRUE), 
  repartition = 4)
```

Nótese que aquí hicimos una partición de los datos explícita (si los
datos están distribuidos, las particiones son implícitas). Cuando
transformamos o filtramos esta tabla, puede haber 4 ejecutores
trabajando en paralelo.

## Básicos de manipulación y análisis

Usamos dplyr y sparklyr para empujar código a spark, donde se ejecutan
los queries y las transformaciones. Usamos el mismo código que usaríamos
en dplyr aplicado a una *tibble* de *dplyr*:

```{r}
perfiles_tbl |> count()
```

-   Revisa ahora [la webUI de Spark](http://0.0.0.0:4040/jobs/).
    Identifica el trabajo que acabamos de correr. Revisa sus dos etapas
    (stages), que corresponden a una agregación dentro de cada partición
    (donde hay una task para cada partición) y luego un shuffle
    (Exchange) para agrupar los resultados de cada partición. Los
    *shuffles* tienden a ser las operaciones más costosas (hay que mover
    datos), y ocurren por ejemplo con *group_by* y *joins*.

Prueba después de correr lo siguiente:

```{r}
resumen <- perfiles_tbl |>  group_by(sex) |> count() 
resumen
```

```{r}
conteo_pets <- perfiles_tbl |>
  mutate(pets = ifelse(is.null(pets), "No disponible", pets)) |> 
  group_by(pets) |>
  summarise(n = n(), edad_media = mean(age), sd_edad = sd(age)) |> 
  mutate(prop = n / sum(n)) |> arrange(desc(prop))
conteo_pets
```

Esto funciona aunque las tablas no están en la sesión de R, sino en
Spark

```{r}
class(perfiles_tbl)
class(conteo_pets)
```

```{r}
glimpse(perfiles_tbl)
```

Nótese: que este código no se ejecuta en la sesión de R, sino que dplyr
envía código traducido en sparksql a sql, y ahí se ejecuta. Puede usarse
group_by, select, mutate, filter. Las funciones que dplyr puede traducir
a Spark SQL [son](https://spark.rstudio.com/dplyr/):

-   Basic math operators +, -, \*, /, %%, \^,
-   Math functions abs, acos, asin, asinh, atan, atan2, ceiling, cos,
    cosh, exp, floor, log, log10, round, sign, sin, sinh, sqrt, tan,
    tanh,
-   Logical comparisons \<, \<=, !=, \>=, \>, ==, %in%,
-   Boolean operations &, &&, \|, \|\|, !,
-   Character functions paste, tolower, toupper, nchar
-   Casting: as.double, as.integer, as.logical, as.character, as.date
-   Basic aggregations: mean, sum, min, max, sd, var, cor, cov, n
    functions

Se pueden hacer también joins (left_join, anti_join, etc), muestrar con
sample_n y sample_frac, usar la función ifelse, y otras.

Cuando dplyr no reconoce una función, se pasa al sql directamente, y así
se pueden usar [funciones de
Hive](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-HiveOperatorsandUser-DefinedFunctions(UDFs)),
por ejemplo

```{r}
fechas <- perfiles_tbl |>
  select(last_online) |> 
  mutate(fecha = substr(last_online, 0, 10),
         hora = substr(last_online, 12, 20)) |>
  mutate(hora = regexp_replace(hora, "-", ":")) |>
  mutate(timestamp = concat_ws(" ", fecha, hora)) |> 
  mutate(dia = dayofmonth(timestamp))
fechas
fechas |> count()
```

```{r}
# código en sql
fechas |> dbplyr::sql_render()
```

-   concat_ws, regexp_replace y dayofmonth son funciones de Hive (ver
    [documentación de
    Hive](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-HiveOperatorsandUser-DefinedFunctions(UDFs))).
-   Estas funciones no son funciones de R

Nótese que creamos particiones en los datos para que nos permita
aprovechar paralelismo (sobre los renglones de los datos)

### Copiando datos resumidos a R

Podemos traer datos a R (por ejemplo para usar ggplot, etc.) usando
*collect*:

```{r}
resultado <- conteo_pets |> collect()
resultado
glimpse(resultado)
```

**Nota**: es mala idea hacer collect de tablas grandes. Los datos se
copian en memoria, y no es un proceso rápido pues hay que traducir de
spark a R. Lo mismo aplica a copiar de R a Spark.

### Evaluación perezosa

En Spark y dplyr se utiliza evaluación perezosa, es decir, se hacen
cálculos solo hasta que se requieren los valores. Esto permite a Spark
planear para optimizar el cálculo. Esto quiere decir que a veces
corremos alguna parte de nuestro código que consiste en tranformaciones
y no hay ninguna acción, el código parece correr muy rápido. En el
momento que hacemos una acción (como tally o collect), entonces el
cálculo se dispara.

Por ejemplo:

```{r}
# el archivo es de unos 3.3Gb, pero esto corre casi instantáneo:
system.time(
  temp <- spark_read_text(sc,
      path = "../../datos/wikipedia/article_categories_en.ttl",
      memory = FALSE) |>
    filter(substr(line, 0, 1) != "#") |> 
    mutate(urls = split(line, " ")) |> 
    select(urls) |>
    hof_transform( ~ regexp_extract(.x, "(?<=/)([^/]+)(?=>)")) |> 
    sdf_separate_column("urls") |> 
    select(urls_1, urls_3) 
)
```

Tarda muy poco porque spark todavía no ha ejecutado código de
transformación ni ha traido los datos a memoria. Acciones son *collect*,
*tally* y *summarise*, por ejemplo. Se puede forzar un cómputo usando la
función *compute*, que crea una tabla de Spark con el argumento que
recibe y la guarda en memoria con un nombre.

Este es rápido porque solo transforma 100 renglones:

```{r}
system.time(temp_tbl <- temp |> head(100) |> collect())
temp_tbl
```

En el siguiente, sin embargo, tenemos que transformar todo para filtrar:


```{r}
system.time(articulo_tbl <- temp |> filter(urls_1 == "Skiing") |> collect())
articulo_tbl
```

```{r}
system.time(categoria_tbl <- temp |> filter(urls_3 == "Category:Skiing") |> collect())
categoria_tbl
```


También sucede si agrupamos:

```{r}
system.time(temp_tbl <- temp |> group_by(urls_3) |> 
              summarise(n_articulos = n()))
system.time(res_tbl <- temp_tbl |> head(100))
res_tbl
```

## Funciones que transforman un DataFrame de Spark

Tenemos otras funciones que sirven para operar directamente en tablas de
Spark sin pasar por dplyr. Muchas de estas empiezan con *sdf\_"*, por
ejemplo:

```{r}
sdf_schema(perfiles_tbl)
sdf_describe(perfiles_tbl)
```

```{r}
perfiles_part <- sdf_random_split(perfiles_tbl, entrena = 0.9, prueba = 0.1)
perfiles_part$prueba |> tally()
perfiles_part$entrena |> tally()
```

Para crear dataframes en spark directamente

```{r}
ejemplo <- sdf_along(sc, along = 1:10)
glimpse(ejemplo)
sdf_register(ejemplo, "ejemplo")
```

-   sdf_broadcast, sdf_checkpoint, sdf_bind_rows, sdf_bind_cols,
    sdf_repartition etc. son otras funciones útiles

## Transformadores y Estimadores

El paquete Spark ML de Spark provee los siguientes conceptos útiles para
trabajar con modelos y predicciones a partir de DataFrames de Spark.

-   Transformadores: toman un DataFrame y producen otro (por ejemplo,
    agregar una columna de predicciones)

-   Estimadores: se ajustan con un DataFrame y resultan en un
    Transformador

-   Parámetros: estimadores y transformadores tiene parámetros que se
    pueden ajustar con un DataFrame o directamente

-   Pipeline: cadena de estimadores y transformadores. Es un estimador,
    y cuando se ajusta produce un transformador.

-   Los transformadores definidos en Spark comienzan con "*ft\_*, puedes
    ver por ejemplo
    [aqui](https://spark.apache.org/docs/2.4.0/ml-features.html). Todos
    los transformadores y estimadores de spark pueden accederse desde
    sparklyr.

-   Los estimadores de machine learning de Spark comienzan con *ml\_*,
    como ml_linear_regression, ml_gradient_boosted_trees,
    ml_bisecting_kmeans, etc.

## Particiones

Spark trabaja con tablas particionadas por renglones. En un cluster con
un sistema de archivos distribuidos, los datos están distribuidos
implícitamente. Podemos también reparticionar explícitamente para
mejorar el desempeño (por ejemplo, si tenemos pocas particiones grandes
y muchos trabajadores o cores) podemos crear más particiones. Si tenemos
muchas particiones chicas y pocos trabajadores grandes, podemos hacer
menos particiones.

Por ejemplo

```{r}
library(microbenchmark)
library(ggplot2)
dat_bench <- microbenchmark(
    "1 Partition(s)" = sdf_len(sc, 10^8, repartition = 1) |>
      summarise(mean(id)) |> collect(),
    "2 Partition(s)" = sdf_len(sc, 10^8, repartition = 10) |>
      summarise(mean(id)) |> collect(),
    times = 2
) |> as_tibble()
dat_bench |> mutate(segundos = time / 10e9)
```

Si vamos a reusar un DataFrame de Spark en varias ocasiones, conviene
leerlo a memoria (usando memory = TRUE), o usando tbl_cache() (ver
Storage en el Spark UI) para no repetir la carga o el cómputo cuando lo
necesitemos:

```{r}
tbl_cache(sc, "perfiles")
perfiles_tbl <- tbl(sc, "perfiles")
tbl_uncache(sc, "perfiles")
```



## Parquet

Y podemos guardar también dataframes de Spark a disco, por ejemplo,
particionamos y escribimos en formato de Parquet:

```{r}
spark_write_parquet(perfiles_tbl |> 
    mutate(a_mes = substr(last_online, 0, 6)),
  path = "../../datos/profiles.parquet",
  partition_by = "a_mes")
```

```{r}
perfiles_2012 <- spark_read_parquet(sc, name = "perfiles_2", 
  path = "../../datos/profiles.parquet/a_mes=2012-*", memory = TRUE, 
  overwrite = TRUE) #|> 
perfiles_2012  |> 
  summarise(last_online_min = min(last_online),
            last_online_max = max(last_online)) |>
  collect()
```

```{r}
spark_disconnect(sc)
```
